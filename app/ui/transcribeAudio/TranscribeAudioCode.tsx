import React from "react";
import { CodeBlock } from "../../../components/ui/code-block";

const frontendCode = `"use client";

import React, { useState, useRef } from "react";
import { Mic, Upload, ArrowLeft, FileAudio } from "lucide-react";
import Link from "next/link";

function TranscribeAudio() {
  const [transcription, setTranscription] = useState<string>("");
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [selectedFile, setSelectedFile] = useState<File | null>(null);
  const fileInputRef = useRef<HTMLInputElement>(null);

  const handleFileSelect = (e: React.ChangeEvent<HTMLInputElement>) => {
    const file = e.target.files?.[0];
    if (file) {
      // Validate file type (audio files)
      const validTypes = ['audio/wav', 'audio/mp3', 'audio/mpeg', 'audio/m4a'];
      if (validTypes.includes(file.type)) {
        setSelectedFile(file);
        setError(null);
      } else {
        setError("Please select a valid audio file (WAV, MP3, M4A)");
        setSelectedFile(null);
      }
    }
  };

  const handleSubmit = async (e: React.FormEvent<HTMLFormElement>) => {
    e.preventDefault();
    if (!selectedFile || isLoading) return;

    setIsLoading(true);
    setError(null);
    setTranscription("");

    try {
      const formData = new FormData();
      formData.append("audio", selectedFile);

      const response = await fetch("/api/transcribe-audio", {
        method: "POST",
        body: formData,
      });

      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(errorData.error || "Failed to transcribe audio");
      }

      const result = await response.json();
      setTranscription(result.transcript || result.text);
    } catch (err) {
      console.error("Transcription error:", err);
      setError("Failed to transcribe audio. Please try again.");
    } finally {
      setIsLoading(false);
    }
  };

  const handleNewTranscription = () => {
    setSelectedFile(null);
    setTranscription("");
    setError(null);
    if (fileInputRef.current) {
      fileInputRef.current.value = "";
    }
  };

  return (
    <div className="flex flex-col h-screen bg-gray-50">
      {/* Header, file upload, and transcription display */}
    </div>
  );
}

export default TranscribeAudio;`;

const backendCode = `import { NextRequest, NextResponse } from "next/server";
import { openai } from "@ai-sdk/openai";
import { experimental_transcribe as transcribe } from "ai";

export async function POST(req: NextRequest) {
  try {
    const formData = await req.formData();
    const audioFile = formData.get("audio") as File;

    if (!audioFile) {
      return NextResponse.json(
        { error: "No audio file provided" },
        { status: 400 }
      );
    }

    const audioBuffer = await audioFile.arrayBuffer();
    const uint8Array = new Uint8Array(audioBuffer);

    const transcription = await transcribe({
      model: openai.transcription("whisper-1"),
      audio: uint8Array,
    });

    return NextResponse.json(transcription);
  } catch (error) {
    return NextResponse.json(
      { error: "Failed to transcribe audio", details: error },
      { status: 500 }
    );
  }
}`;

export function TranscribeAudioCode() {
  return (
    <div className="max-w-4xl mx-auto p-6 space-y-8">
      <div className="text-center mb-8">
        <h2 className="text-3xl font-bold text-gray-900 mb-4">
          Implementation Code
        </h2>
        <p className="text-gray-600">
          Here&apos;s how the Audio Transcription feature is built using AI SDK
          and Whisper
        </p>
      </div>

      <div className="space-y-6">
        <div>
          <h3 className="text-xl font-semibold text-gray-800 mb-4 flex items-center gap-2">
            <div className="w-2 h-2 bg-blue-500 rounded-full"></div>
            Frontend Implementation
          </h3>
          <CodeBlock
            language="tsx"
            filename="app/ui/transcribeAudio/page.tsx"
            code={frontendCode}
          />
        </div>

        <div>
          <h3 className="text-xl font-semibold text-gray-800 mb-4 flex items-center gap-2">
            <div className="w-2 h-2 bg-green-500 rounded-full"></div>
            Backend API Route
          </h3>
          <CodeBlock
            language="typescript"
            filename="app/api/transcribe-audio/route.ts"
            code={backendCode}
          />
        </div>

        <div className="bg-green-50 border border-green-200 rounded-lg p-6">
          <h4 className="text-lg font-semibold text-green-800 mb-3">
            Key Features
          </h4>
          <ul className="space-y-2 text-green-700">
            <li className="flex items-start gap-2">
              <div className="w-1.5 h-1.5 bg-green-500 rounded-full mt-2 flex-shrink-0"></div>
              <span>
                OpenAI Whisper-1 model for accurate speech recognition
              </span>
            </li>
            <li className="flex items-start gap-2">
              <div className="w-1.5 h-1.5 bg-green-500 rounded-full mt-2 flex-shrink-0"></div>
              <span>
                File upload with validation for audio formats (WAV, MP3, M4A)
              </span>
            </li>
            <li className="flex items-start gap-2">
              <div className="w-1.5 h-1.5 bg-green-500 rounded-full mt-2 flex-shrink-0"></div>
              <span>FormData handling for binary audio file uploads</span>
            </li>
            <li className="flex items-start gap-2">
              <div className="w-1.5 h-1.5 bg-green-500 rounded-full mt-2 flex-shrink-0"></div>
              <span>ArrayBuffer to Uint8Array conversion for AI SDK</span>
            </li>
          </ul>
        </div>
      </div>
    </div>
  );
}
